{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Secure Water Treatment (SWaT)\n",
    "\n",
    "- Source and description: https://itrust.sutd.edu.sg/itrust-labs_datasets/dataset_info/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-15T03:18:00.845554Z",
     "start_time": "2024-10-15T03:17:55.359529Z"
    }
   },
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import data_raw_folder, data_processed_folder\n",
    "\n",
    "from timeeval import Datasets, DatasetManager\n",
    "from timeeval.datasets import DatasetAnalyzer, DatasetRecord\n",
    "\n",
    "try:\n",
    "    from openpyxl import Workbook\n",
    "except ImportError:\n",
    "    import sys\n",
    "    !conda install --yes --prefix {sys.prefix} openpyxl\n",
    "    from openpyxl import Workbook"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-15T03:18:00.852883Z",
     "start_time": "2024-10-15T03:18:00.848093Z"
    }
   },
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20, 10)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-15T03:18:01.569785Z",
     "start_time": "2024-10-15T03:18:00.855315Z"
    }
   },
   "source": [
    "source_folder = Path(data_raw_folder) \n",
    "target_folder = Path(data_processed_folder)\n",
    "\n",
    "path_mapping = {\n",
    "    \"SWaT-A1&A2\": (\n",
    "        source_folder / \"SWaT.A1&A2_Dec2015\" / \"Physical\" / \"SWaT_Dataset_Attack_v0.xlsx\",  # test\n",
    "        source_folder / \"SWaT.A1&A2_Dec2015\" / \"Physical\" / \"SWaT_Dataset_Normal_v1.xlsx\",  # train\n",
    "    ),\n",
    "    # \"SWaT-A3\": \"\",  # not relevant\n",
    "    # \"SWaT-A4\": \"SWaT.A4&A5_Jul 2019\",  # use A5\n",
    "    \"SWaT-A5\": source_folder / \"SWaT.A4&A5_Jul2019\" / \"SWaT_dataset_Jul19v2.xlsx\",  # only test = unsupervised\n",
    "    \"SWaT-A6\": source_folder / \"SWaT.A6_Dec2019\" / \"csv\" / \"Dec2019.xlsx\",  # only test = unsupervised\n",
    "    # \"SWaT-A7\": \"\",  # not relevant\n",
    "    # \"SWaT-A8\": \"\",  # note relevant\n",
    "    \n",
    "}\n",
    "\n",
    "# define anomalies\n",
    "anomalies_A5 = {\n",
    "    \"Attack1\": {\n",
    "        \"description\": \"Spoof value of FIT401 from 0.8 to 0.5 to stop de-chlorination by switching off UV401\",\n",
    "        \"targets\": [\"FIT 401\"],\n",
    "        \"begin\": \"2019-07-20 15:08:46+08:00\",\n",
    "        \"end\": \"2019-07-20 15:10:31+08:00\"\n",
    "    },\n",
    "    \"Attack2\": {\n",
    "        \"description\": \" Spoof value of LIT301 from 835 to 1024 to eventually lead to underflow in T301\",\n",
    "        \"targets\": [\"LIT 301\"],\n",
    "        \"begin\": \"2019-07-20 15:15:00+08:00\",\n",
    "        \"end\": \"2019-07-20 15:19:32+08:00\"\n",
    "    },\n",
    "    \"Attack3\": {\n",
    "        \"description\": \"Switch P601 from OFF to ON to increase water in raw water tank\",\n",
    "        \"targets\": [\"P601 Status\"],\n",
    "        \"begin\": \"2019-07-20 15:26:57+08:00\",\n",
    "        \"end\": \"2019-07-20 15:30:48+08:00\"\n",
    "    },\n",
    "    \"Attack4\": {\n",
    "        \"description\": \"Switch from CLOSE to OPEN (MV201) and OFF to ON (P101) to overflow tank T301\",\n",
    "        \"targets\": [\"MV201\", \"P101 Status\"],\n",
    "        \"begin\": \"2019-07-20 15:38:50+08:00\",\n",
    "        \"end\": \"2019-07-20 15:46:20+08:00\"\n",
    "    },\n",
    "    \"Attack5\": {\n",
    "        \"description\": \"Switch MV501 from OPEN to CLOSE to drain water from RO\",\n",
    "        \"targets\": [\"MV 501\"],\n",
    "        \"begin\": \"2019-07-20 15:54:00+08:00\",\n",
    "        \"end\": \"2019-07-20 15:56:00+08:00\"\n",
    "    },\n",
    "    \"Attack6\": {\n",
    "        \"description\": \"Switch P301 from ON to OFF to halt stage 3 (UF process)\",\n",
    "        \"targets\": [\"P301 Status\"],\n",
    "        \"begin\": \"2019-07-20 16:02:56+08:00\",\n",
    "        \"end\": \"2019-07-20 16:16:18+08:00\"\n",
    "    },\n",
    "}\n",
    "\n",
    "for a in anomalies_A5:\n",
    "    for dt_key in [\"begin\", \"end\"]:\n",
    "        dt = anomalies_A5[a][dt_key]\n",
    "        # convert times to UTZ-datetimes\n",
    "        dt = pd.to_datetime(dt).astimezone(tz=\"UTC\")\n",
    "        \n",
    "        # fix anomaly labels (timestamps)\n",
    "        dt = dt - pd.Timedelta(2, unit=\"m\") + pd.Timedelta(14, unit=\"s\")\n",
    "        anomalies_A5[a][dt_key] = dt\n",
    "\n",
    "    if a == \"Attack1\":\n",
    "        anomalies_A5[a][\"begin\"] -= pd.Timedelta(2, unit=\"s\")\n",
    "        anomalies_A5[a][\"end\"] -= pd.Timedelta(1, unit=\"s\")\n",
    "    elif a == \"Attack2\":\n",
    "        anomalies_A5[a][\"begin\"] += pd.Timedelta(5, unit=\"s\")\n",
    "    elif a == \"Attack4\":\n",
    "        anomalies_A5[a][\"end\"] += pd.Timedelta(30, unit=\"s\")\n",
    "    elif a == \"Attack5\":\n",
    "        anomalies_A5[a][\"begin\"] += pd.Timedelta(3, unit=\"s\")\n",
    "        anomalies_A5[a][\"end\"] += pd.Timedelta(40, unit=\"s\")\n",
    "    elif a == \"Attack6\":\n",
    "        anomalies_A5[a][\"begin\"] -= pd.Timedelta(5, unit=\"s\")\n",
    "        anomalies_A5[a][\"end\"] += pd.Timedelta(35, unit=\"s\")\n",
    "\n",
    "anomalies_A6 = {\n",
    "    \"C2Com&Attack5\": {\n",
    "        \"description\": \"Infiltrate SCADA WS with second payload from C2, and disrupt sensors cycle 1\",\n",
    "        \"begin\": \"12:30\",\n",
    "        \"end\": \"12:33\"\n",
    "    },\n",
    "    \"Attack6\": {\n",
    "        \"description\": \"Disrupt sensors cycle 2\",\n",
    "        \"begin\": \"12:43\",\n",
    "        \"end\": \"12:46\"\n",
    "    },\n",
    "    \"Attack7\": {\n",
    "        \"description\": \"Disrupt sensors cycle 3\",\n",
    "        \"begin\": \"12:56\",\n",
    "        \"end\": \"12:59\"\n",
    "    },\n",
    "    \"Attack8\": {\n",
    "        \"description\": \"Disrupt sensors cycle 4\",\n",
    "        \"begin\": \"13:09\",\n",
    "        \"end\": \"13:12\"\n",
    "    },\n",
    "    \"Attack9\": {\n",
    "        \"description\": \"Disrupt sensors cycle 5\",\n",
    "        \"begin\": \"13:22\",\n",
    "        \"end\": \"13:25\"\n",
    "    }\n",
    "}\n",
    "\n",
    "date = \"2019-12-06\"\n",
    "tz = \"Asia/Hong_Kong\"\n",
    "for a in anomalies_A6:\n",
    "    for dt_key in [\"begin\", \"end\"]:\n",
    "        # convert times to datetimes\n",
    "        dt = anomalies_A6[a][dt_key]\n",
    "        dt = pd.to_datetime(f\"{date} {dt}:00\").tz_localize(tz=tz)\n",
    "        anomalies_A6[a][dt_key] = dt\n",
    "\n",
    "print(f\"Looking for source datasets in {Path(source_folder).absolute()} and\\nsaving processed datasets in {Path(target_folder).absolute()}\")"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-10-15T03:18:01.997583Z",
     "start_time": "2024-10-15T03:18:01.572342Z"
    }
   },
   "source": [
    "# shared by all datasets\n",
    "dataset_collection_name = \"SWaT\"\n",
    "dataset_type = \"real\"\n",
    "input_type = \"multivariate\"\n",
    "datetime_index = True\n",
    "split_at = None\n",
    "\n",
    "# create target subfolder\n",
    "dataset_subfolder = Path(input_type) / dataset_collection_name\n",
    "target_subfolder = target_folder / dataset_subfolder\n",
    "target_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created directories {target_subfolder}\")\n",
    "\n",
    "dm = DatasetManager(target_folder)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T03:18:02.010492Z",
     "start_time": "2024-10-15T03:18:01.999553Z"
    }
   },
   "source": [
    "def add_anomalies(df: pd.DataFrame, anomalies: dict) -> pd.DataFrame:\n",
    "    df = df.set_index(\"timestamp\")\n",
    "    df[\"is_anomaly\"] = 0\n",
    "    for a in anomalies:\n",
    "        begin = anomalies[a][\"begin\"]\n",
    "        end = anomalies[a][\"end\"]\n",
    "        df.loc[begin:end, \"is_anomaly\"] = 1\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "def prepare_dataset_A1_A2(dataset_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_excel(\n",
    "        dataset_path,\n",
    "        sheet_name=0,\n",
    "        skiprows=1,\n",
    "        header=0,\n",
    "        index_col=None,\n",
    "        parse_dates=[0]\n",
    "    )\n",
    "    df = df.rename(columns={\" Timestamp\": \"timestamp\"})\n",
    "    df[\"is_anomaly\"] = 0\n",
    "    df.loc[df[\"Normal/Attack\"] == \"Attack\", \"is_anomaly\"] = 1\n",
    "    df = df.drop(columns=[\"Normal/Attack\"])\n",
    "    return df\n",
    "\n",
    "def prepare_dataset_A5(dataset_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_excel(\n",
    "        dataset_path,\n",
    "        sheet_name=0,\n",
    "        skiprows=1,\n",
    "        header=[0, 1],\n",
    "        index_col=None,\n",
    "        parse_dates=[0]\n",
    "    )\n",
    "    # align header\n",
    "    df.columns = list(df.columns.get_level_values(1)[0:1]) + list(df.columns.get_level_values(0)[1:])\n",
    "\n",
    "    # round timestamps to have equi-distant steps (= seconds)\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].map(lambda dt: dt.floor(freq=\"S\"))\n",
    "\n",
    "    # make actuator status numeric\n",
    "    df = df.replace(\"Inactive\", 0)\n",
    "    df = df.replace(\"Active\", 1)\n",
    "\n",
    "    # add anomaly labels\n",
    "    df = add_anomalies(df, anomalies_A5)\n",
    "    return df\n",
    "\n",
    "def prepare_dataset_A6(dataset_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_excel(\n",
    "        dataset_path,\n",
    "        sheet_name=0,\n",
    "        skiprows=9,\n",
    "        header=0,\n",
    "        index_col=None,\n",
    "        parse_dates=[0]\n",
    "    )\n",
    "\n",
    "    # localize and convert timestamp\n",
    "    s = df[\"t_stamp\"]\n",
    "    s = s.map(lambda dt: dt.tz_localize(tz=\"Asia/Hong_Kong\"))\n",
    "    df.insert(0, \"timestamp\", s)\n",
    "    df.drop(columns=[\"t_stamp\"], inplace=True)\n",
    "\n",
    "    # make actuator status numeric\n",
    "    df = df.replace(\"Inactive\", 0)\n",
    "    df = df.replace(\"Active\", 1)\n",
    "\n",
    "    # add anomaly labels\n",
    "    df = add_anomalies(df, anomalies_A6)\n",
    "    return df"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A1 and A2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T08:06:29.973936Z",
     "start_time": "2024-09-24T08:00:09.012656Z"
    }
   },
   "source": [
    "dataset_name = \"SWaT-A1&A2\"\n",
    "train_type = \"semi-supervised\"\n",
    "train_is_normal = True\n",
    "\n",
    "print(f\"-> Processing dataset {dataset_name}\")\n",
    "\n",
    "test_filename = f\"{dataset_name}.test.csv\"\n",
    "train_filename = f\"{dataset_name}.train.csv\"\n",
    "test_path = dataset_subfolder / test_filename\n",
    "train_path = dataset_subfolder / train_filename\n",
    "target_test_filepath = target_subfolder / test_filename\n",
    "target_train_filepath = target_subfolder / train_filename\n",
    "target_meta_filepath = target_test_filepath.parent / f\"{dataset_name}.{Datasets.METADATA_FILENAME_SUFFIX}\"\n",
    "\n",
    "# prepare test dataset\n",
    "source_paths = path_mapping[dataset_name]\n",
    "df_test = prepare_dataset_A1_A2(source_paths[0])\n",
    "df_test.to_csv(target_test_filepath, index=False)\n",
    "print(\"  written test dataset\")\n",
    "\n",
    "da = DatasetAnalyzer((dataset_collection_name, dataset_name), is_train=False, df=df_test, ignore_stationarity=True)\n",
    "da.save_to_json(target_meta_filepath, overwrite=True)\n",
    "meta = da.metadata\n",
    "del da\n",
    "del df_test\n",
    "print(\"  analyzed test dataset\")\n",
    "\n",
    "# prepare train dataset\n",
    "df_train = prepare_dataset_A1_A2(source_paths[1])\n",
    "df_train.to_csv(target_train_filepath, index=False)\n",
    "print(f\"  written training dataset\")\n",
    "\n",
    "DatasetAnalyzer((dataset_collection_name, dataset_name), is_train=True, df=df_train, ignore_stationarity=True)\\\n",
    "    .save_to_json(target_meta_filepath, overwrite=False)\n",
    "del df_train\n",
    "print(f\"  analyzed training dataset\")\n",
    "\n",
    "dm.add_dataset(DatasetRecord(\n",
    "      collection_name=dataset_collection_name,\n",
    "      dataset_name=dataset_name,\n",
    "      train_path=train_path,\n",
    "      test_path=test_path,\n",
    "      dataset_type=dataset_type,\n",
    "      datetime_index=datetime_index,\n",
    "      split_at=split_at,\n",
    "      train_type=train_type,\n",
    "      train_is_normal=train_is_normal,\n",
    "      input_type=input_type,\n",
    "      length=meta.length,\n",
    "      dimensions=meta.dimensions,\n",
    "      contamination=meta.contamination,\n",
    "      num_anomalies=meta.num_anomalies,\n",
    "      min_anomaly_length=meta.anomaly_length.min,\n",
    "      median_anomaly_length=meta.anomaly_length.median,\n",
    "      max_anomaly_length=meta.anomaly_length.max,\n",
    "      mean=meta.mean,\n",
    "      stddev=meta.stddev,\n",
    "      trend=meta.trend,\n",
    "      stationarity=meta.get_stationarity_name(),\n",
    "      period_size=np.nan\n",
    "))\n",
    "print(f\"... processed dataset {dataset_name}\")\n"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A5"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T03:21:44.761247Z",
     "start_time": "2024-10-15T03:19:02.184051Z"
    }
   },
   "source": [
    "dataset_name = \"SWaT-A5\"\n",
    "train_type = \"unsupervised\"\n",
    "train_is_normal = False\n",
    "\n",
    "print(f\"-> Processing dataset {dataset_name}\")\n",
    "\n",
    "test_filename = f\"{dataset_name}.test.csv\"\n",
    "test_path = dataset_subfolder / test_filename\n",
    "target_test_filepath = target_subfolder / test_filename\n",
    "target_meta_filepath = target_test_filepath.parent / f\"{dataset_name}.{Datasets.METADATA_FILENAME_SUFFIX}\"\n",
    "\n",
    "# prepare test dataset\n",
    "source_path = path_mapping[dataset_name]\n",
    "df_test = prepare_dataset_A5(source_path)\n",
    "df_test.to_csv(target_test_filepath, index=False)\n",
    "print(\"  written test dataset\")\n",
    "\n",
    "da = DatasetAnalyzer((dataset_collection_name, dataset_name), is_train=False, df=df_test)\n",
    "da.save_to_json(target_meta_filepath, overwrite=True)\n",
    "meta = da.metadata\n",
    "del da\n",
    "del df_test\n",
    "print(\"  analyzed test dataset\")\n",
    "\n",
    "dm.add_dataset(DatasetRecord(\n",
    "      collection_name=dataset_collection_name,\n",
    "      dataset_name=dataset_name,\n",
    "      train_path=None,\n",
    "      test_path=test_path,\n",
    "      dataset_type=dataset_type,\n",
    "      datetime_index=datetime_index,\n",
    "      split_at=split_at,\n",
    "      train_type=train_type,\n",
    "      train_is_normal=train_is_normal,\n",
    "      input_type=input_type,\n",
    "      length=meta.length,\n",
    "      dimensions=meta.dimensions,\n",
    "      contamination=meta.contamination,\n",
    "      num_anomalies=meta.num_anomalies,\n",
    "      min_anomaly_length=meta.anomaly_length.min,\n",
    "      median_anomaly_length=meta.anomaly_length.median,\n",
    "      max_anomaly_length=meta.anomaly_length.max,\n",
    "      mean=meta.mean,\n",
    "      stddev=meta.stddev,\n",
    "      trend=meta.trend,\n",
    "      stationarity=meta.get_stationarity_name(),\n",
    "      period_size=np.nan\n",
    "))\n",
    "print(f\"... processed dataset {dataset_name}\")"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A6"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset_name = \"SWaT-A6\"\n",
    "train_type = \"unsupervised\"\n",
    "train_is_normal = False\n",
    "\n",
    "print(f\"-> Processing dataset {dataset_name}\")\n",
    "\n",
    "test_filename = f\"{dataset_name}.test.csv\"\n",
    "test_path = dataset_subfolder / test_filename\n",
    "target_test_filepath = target_subfolder / test_filename\n",
    "target_meta_filepath = target_test_filepath.parent / f\"{dataset_name}.{Datasets.METADATA_FILENAME_SUFFIX}\"\n",
    "\n",
    "# prepare test dataset\n",
    "source_path = path_mapping[dataset_name]\n",
    "df_test = prepare_dataset_A6(source_path)\n",
    "df_test.to_csv(target_test_filepath, index=False)\n",
    "print(\"  written test dataset\")\n",
    "\n",
    "da = DatasetAnalyzer((dataset_collection_name, dataset_name), is_train=False, df=df_test)\n",
    "da.save_to_json(target_meta_filepath, overwrite=True)\n",
    "meta = da.metadata\n",
    "del da\n",
    "del df_test\n",
    "print(\"  analyzed test dataset\")\n",
    "\n",
    "dm.add_dataset(DatasetRecord(\n",
    "      collection_name=dataset_collection_name,\n",
    "      dataset_name=dataset_name,\n",
    "      train_path=None,\n",
    "      test_path=test_path,\n",
    "      dataset_type=dataset_type,\n",
    "      datetime_index=datetime_index,\n",
    "      split_at=split_at,\n",
    "      train_type=train_type,\n",
    "      train_is_normal=train_is_normal,\n",
    "      input_type=input_type,\n",
    "      length=meta.length,\n",
    "      dimensions=meta.dimensions,\n",
    "      contamination=meta.contamination,\n",
    "      num_anomalies=meta.num_anomalies,\n",
    "      min_anomaly_length=meta.anomaly_length.min,\n",
    "      median_anomaly_length=meta.anomaly_length.median,\n",
    "      max_anomaly_length=meta.anomaly_length.max,\n",
    "      mean=meta.mean,\n",
    "      stddev=meta.stddev,\n",
    "      trend=meta.trend,\n",
    "      stationarity=meta.get_stationarity_name(),\n",
    "      period_size=np.nan\n",
    "))\n",
    "print(f\"... processed dataset {dataset_name}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T08:11:09.948432Z",
     "start_time": "2024-09-24T08:11:09.939838Z"
    }
   },
   "source": [
    "dm.save()"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-09-24T08:11:17.138772Z",
     "start_time": "2024-09-24T08:11:17.029825Z"
    }
   },
   "source": [
    "dm.refresh()\n",
    "dm.df().loc[(slice(dataset_collection_name,dataset_collection_name), slice(None))]"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exploration\n",
    "\n",
    "### A1 and A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "df = pd.read_excel(\n",
    "    source_folder / \"SWaT.A1&A2_Dec 2015\" / \"Physical\" / \"SWaT_Dataset_Attack_v0.xlsx\",\n",
    "    sheet_name=0,\n",
    "    skiprows=1,\n",
    "    header=0,\n",
    "    index_col=None,\n",
    "    parse_dates=[0]\n",
    ")\n",
    "df[\"is_anomaly\"] = 0\n",
    "df.loc[df[\"Normal/Attack\"] == \"Attack\", \"is_anomaly\"] = 1\n",
    "df = df.drop(columns=[\"Normal/Attack\"])\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "df_tmp = df[[\"FIT101\", \"AIT203\", \"AIT203\", \"PIT503\", \"is_anomaly\"]]\n",
    "df_tmp.iloc[:, :-1].plot()\n",
    "s = df_tmp[\"is_anomaly\"].diff()\n",
    "for begin, end in zip(s[s == -1].index, s[s == 1].index):\n",
    "    plt.gca().add_patch(matplotlib.patches.Rectangle((begin, 0), end - begin, df_tmp.max().max(), color=\"red\", alpha=0.25))\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "df = pd.read_excel(\n",
    "    source_folder / \"SWaT.A1&A2_Dec 2015\" / \"Physical\" / \"SWaT_Dataset_Normal_v1.xlsx\",\n",
    "    sheet_name=0,\n",
    "    skiprows=1,\n",
    "    header=0,\n",
    "    index_col=None,\n",
    "    parse_dates=[0]\n",
    ")\n",
    "df[\"is_anomaly\"] = 0\n",
    "df.loc[df[\"Normal/Attack\"] == \"Attack\", \"is_anomaly\"] = 1\n",
    "df = df.drop(columns=[\"Normal/Attack\"])\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "df_tmp = df[[\"FIT101\", \"LIT101\", \"AIT203\", \"FIT201\", \"LIT401\", \"PIT503\", \"is_anomaly\"]]\n",
    "df_tmp.iloc[:, :-1].plot()\n",
    "s = df_tmp[\"is_anomaly\"].diff()\n",
    "for begin, end in zip(s[s == -1].index, s[s == 1].index):\n",
    "    plt.gca().add_patch(matplotlib.patches.Rectangle((begin, 0), end - begin, df_tmp.max().max(), color=\"red\", alpha=0.25))\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "df = pd.read_excel(\n",
    "    source_folder / \"SWaT.A4&A5_Jul 2019\" / \"SWaT_dataset_Jul 19 v2.xlsx\",\n",
    "    sheet_name=0,\n",
    "    skiprows=1,\n",
    "    header=[0, 1],\n",
    "    index_col=None,\n",
    "    parse_dates=[0]\n",
    ")\n",
    "\n",
    "# align header\n",
    "df.columns = list(df.columns.get_level_values(1)[0:1]) + list(df.columns.get_level_values(0)[1:])\n",
    "\n",
    "# round timestamps to have equi-distant timesteps (= seconds)\n",
    "df[\"timestamp\"] = df[\"timestamp\"].map(lambda dt: dt.floor(freq=\"S\"))\n",
    "\n",
    "df = df.replace(\"Inactive\", 0)\n",
    "df = df.replace(\"Active\", 1)\n",
    "\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "# define anomalies\n",
    "anomalies = {\n",
    "    \"Attack1\": {\n",
    "        \"description\": \"Spoof value of FIT401 from 0.8 to 0.5 to stop de-chlorination by switching off UV401\",\n",
    "        \"targets\": [\"FIT 401\"],\n",
    "        \"begin\": \"2019-07-20 15:08:46+08:00\",\n",
    "        \"end\": \"2019-07-20 15:10:31+08:00\"\n",
    "    },\n",
    "    \"Attack2\": {\n",
    "        \"description\": \" Spoof value of LIT301 from 835 to 1024 to eventually lead to underflow in T301\",\n",
    "        \"targets\": [\"LIT 301\"],\n",
    "        \"begin\": \"2019-07-20 15:15:00+08:00\",\n",
    "        \"end\": \"2019-07-20 15:19:32+08:00\"\n",
    "    },\n",
    "    \"Attack3\": {\n",
    "        \"description\": \"Switch P601 from OFF to ON to increase water in raw water tank\",\n",
    "        \"targets\": [\"P601 Status\"],\n",
    "        \"begin\": \"2019-07-20 15:26:57+08:00\",\n",
    "        \"end\": \"2019-07-20 15:30:48+08:00\"\n",
    "    },\n",
    "    \"Attack4\": {\n",
    "        \"description\": \"Switch from CLOSE to OPEN (MV201) and OFF to ON (P101) to overflow tank T301\",\n",
    "        \"targets\": [\"MV201\", \"P101 Status\"],\n",
    "        \"begin\": \"2019-07-20 15:38:50+08:00\",\n",
    "        \"end\": \"2019-07-20 15:46:20+08:00\"\n",
    "    },\n",
    "    \"Attack5\": {\n",
    "        \"description\": \"Switch MV501 from OPEN to CLOSE to drain water from RO\",\n",
    "        \"targets\": [\"MV 501\"],\n",
    "        \"begin\": \"2019-07-20 15:54:00+08:00\",\n",
    "        \"end\": \"2019-07-20 15:56:00+08:00\"\n",
    "    },\n",
    "    \"Attack6\": {\n",
    "        \"description\": \"Switch P301 from ON to OFF to halt stage 3 (UF process)\",\n",
    "        \"targets\": [\"P301 Status\"],\n",
    "        \"begin\": \"2019-07-20 16:02:56+08:00\",\n",
    "        \"end\": \"2019-07-20 16:16:18+08:00\"\n",
    "    },\n",
    "}\n",
    "\n",
    "for a in anomalies:\n",
    "    for dt_key in [\"begin\", \"end\"]:\n",
    "        # convert times to UTZ-datetimes\n",
    "        dt = pd.to_datetime(anomalies[a][dt_key]).astimezone(tz=\"UTC\")\n",
    "        \n",
    "        # fix anomaly labels (timestamps)\n",
    "        dt = dt - pd.Timedelta(2, unit=\"m\") + pd.Timedelta(14, unit=\"s\")\n",
    "        anomalies[a][dt_key] = dt\n",
    "    if a == \"Attack1\":\n",
    "        anomalies[a][\"begin\"] -= pd.Timedelta(2, unit=\"s\")\n",
    "        anomalies[a][\"end\"] -= pd.Timedelta(1, unit=\"s\")\n",
    "    elif a == \"Attack2\":\n",
    "        anomalies[a][\"begin\"] += pd.Timedelta(5, unit=\"s\")\n",
    "    elif a == \"Attack4\":\n",
    "        anomalies[a][\"end\"] += pd.Timedelta(30, unit=\"s\")\n",
    "    elif a == \"Attack5\":\n",
    "        anomalies[a][\"begin\"] += pd.Timedelta(3, unit=\"s\")\n",
    "        anomalies[a][\"end\"] += pd.Timedelta(40, unit=\"s\")\n",
    "    elif a == \"Attack6\":\n",
    "        anomalies[a][\"begin\"] -= pd.Timedelta(5, unit=\"s\")\n",
    "        anomalies[a][\"end\"] += pd.Timedelta(35, unit=\"s\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "df = df.set_index(\"timestamp\")\n",
    "\n",
    "df[\"is_anomaly\"] = 0\n",
    "\n",
    "for a in anomalies:\n",
    "    begin = anomalies[a][\"begin\"]\n",
    "    end = anomalies[a][\"end\"]\n",
    "    df.loc[begin:end, \"is_anomaly\"] = 1\n",
    "df = df.reset_index()\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "enlarge_x = 120\n",
    "for i in range(6):\n",
    "    a = list(anomalies.keys())[i]\n",
    "    a = anomalies[a]\n",
    "    targets = a[\"targets\"]\n",
    "    begin = a[\"begin\"] - pd.Timedelta(enlarge_x, unit=\"s\")\n",
    "    end = a[\"end\"] + pd.Timedelta(enlarge_x, unit=\"s\")\n",
    "\n",
    "    df_tmp = df.set_index(\"timestamp\")\n",
    "    df_tmp = df_tmp.loc[begin:end, targets + [\"is_anomaly\"]]\n",
    "    df_tmp.iloc[:, :-1].plot()\n",
    "    s = df_tmp[\"is_anomaly\"].diff()\n",
    "    for begin, end in zip(s[s == -1].index, s[s == 1].index):\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((begin, 0), end - begin, df_tmp.max().max(), color=\"red\", alpha=0.25))\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "df = pd.read_excel(\n",
    "    source_folder / \"SWaT.A6_Dec 2019\" / \"csv\" / \"Dec2019.xlsx\",\n",
    "    sheet_name=0,\n",
    "    skiprows=9,\n",
    "    header=0,\n",
    "    index_col=None,\n",
    "    parse_dates=[0]\n",
    ")\n",
    "\n",
    "# localize timestamp\n",
    "s = df[\"t_stamp\"]\n",
    "s = s.map(lambda dt: dt.tz_localize(tz=\"Asia/Hong_Kong\"))\n",
    "df.insert(0, \"timestamp\", s)\n",
    "df.drop(columns=[\"t_stamp\"], inplace=True)\n",
    "\n",
    "df = df.replace(\"Inactive\", 0)\n",
    "df = df.replace(\"Active\", 1)\n",
    "\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "anomalies = {\n",
    "# ignore those attacks, because we could not find any evidence in the sensor data\n",
    "#    \"Infection\": {\n",
    "#        \"description\": \"Infiltrate SCADA WS via USB\",\n",
    "#        \"begin\": \"10:20\",\n",
    "#        \"end\": \"10:30\"\n",
    "#    },\n",
    "#    \"Attack1\": {\n",
    "#        \"description\": \"Exfiltrate data cycle 1\",\n",
    "#        \"begin\": \"10:30\",\n",
    "#        \"end\": \"10:35\"\n",
    "#    },\n",
    "#    \"Attack2\": {\n",
    "#        \"description\": \"Exfiltrate data cycle 2\",\n",
    "#        \"begin\": \"10:45\",\n",
    "#        \"end\": \"10:50\"\n",
    "#    },\n",
    "#    \"Attack3\": {\n",
    "#        \"description\": \"Exfiltrate data cycle 3\",\n",
    "#        \"begin\": \"11:00\",\n",
    "#        \"end\": \"11:05\"\n",
    "#    },\n",
    "#    \"Attack4\": {\n",
    "#        \"description\": \"Exfiltrate data cycle 4\",\n",
    "#        \"begin\": \"11:15\",\n",
    "#        \"end\": \"11:20\"\n",
    "#    },\n",
    "    \"C2Com&Attack5\": {\n",
    "        \"description\": \"Infiltrate SCADA WS with second payload from C2, and disrupt sensors cycle 1\",\n",
    "        \"begin\": \"12:30\",\n",
    "        \"end\": \"12:33\"\n",
    "    },\n",
    "    \"Attack6\": {\n",
    "        \"description\": \"Disrupt sensors cycle 2\",\n",
    "        \"begin\": \"12:43\",\n",
    "        \"end\": \"12:46\"\n",
    "    },\n",
    "    \"Attack7\": {\n",
    "        \"description\": \"Disrupt sensors cycle 3\",\n",
    "        \"begin\": \"12:56\",\n",
    "        \"end\": \"12:59\"\n",
    "    },\n",
    "    \"Attack8\": {\n",
    "        \"description\": \"Disrupt sensors cycle 4\",\n",
    "        \"begin\": \"13:09\",\n",
    "        \"end\": \"13:12\"\n",
    "    },\n",
    "    \"Attack9\": {\n",
    "        \"description\": \"Disrupt sensors cycle 5\",\n",
    "        \"begin\": \"13:22\",\n",
    "        \"end\": \"13:25\"\n",
    "    }\n",
    "}\n",
    "\n",
    "date = \"2019-12-06\"\n",
    "tz = \"Asia/Hong_Kong\"\n",
    "\n",
    "for a in anomalies:\n",
    "    for dt_key in [\"begin\", \"end\"]:\n",
    "        # convert times to datetimes\n",
    "        dt = pd.to_datetime(f\"{date} {anomalies[a][dt_key]}:00\").tz_localize(tz=tz)\n",
    "        anomalies[a][dt_key] = dt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "df = df.set_index(\"timestamp\")\n",
    "\n",
    "df[\"is_anomaly\"] = 0\n",
    "\n",
    "for a in anomalies:\n",
    "    begin = anomalies[a][\"begin\"]\n",
    "    end = anomalies[a][\"end\"]\n",
    "    df.loc[begin:end, \"is_anomaly\"] = 1\n",
    "df = df.reset_index()\n",
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "columns = [c for c in df.columns if c.endswith(\"Pv\")][:5]\n",
    "df_tmp = df.loc[7000:, columns].copy()\n",
    "plt.Figure()\n",
    "df_tmp.plot()\n",
    "s = df[\"is_anomaly\"].diff()\n",
    "for begin, end in zip(s[s == -1].index, s[s == 1].index):\n",
    "    plt.gca().add_patch(matplotlib.patches.Rectangle((begin, 0), end - begin, df_tmp.max().max(), color=\"red\", alpha=0.25))\n",
    "plt.gca().set_ylim(0, 5)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
